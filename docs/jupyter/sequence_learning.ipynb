{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sequence learning**\n",
    "\n",
    "The problem of sequence learning is major problem of intelligence.\n",
    "There are four major sequence problems that have to be solved.\n",
    "\n",
    "Here are the definition of them :\n",
    "\n",
    "> from Ron Sun, C. Lee Giles : \"Sequence Learning\"\n",
    "\n",
    "  1. **Sequence prediction**\n",
    "  \n",
    "   The goal is to predict the next element.\n",
    "   \n",
    "   s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> → s<sub>j+1</sub>, where 1 ≤ i ≤ j < ∞; that is, given s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub>, we want to predict s<sub>j+1</sub>. When i = 1, we make predictions based on all of the previously seen elements of the sequence.\n",
    "When i = j, we make predictions based only on the immediately preceding element.\n",
    "  \n",
    "  2. **Sequence generation**\n",
    "  \n",
    "     The goal is to generate the next element.\n",
    "  \n",
    "   s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> → s<sub>j+1</sub> where 1 ≤ i ≤ j < ∞; that is, given s<sub>i</sub>, s<sub>i+1</sub>, we want to generate s<sub>j+1</sub>. (Put in this way, it is clear that sequence prediction and generation are essentially the same task.)  \n",
    "  \n",
    "  3. **Sequence recognition**\n",
    "  \n",
    "     The goal is to generate correct next element. This is akin to Classification.\n",
    "\n",
    "   s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> → yes or no, where 1 ≤ i ≤ j < ∞;\n",
    "that is, given s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub>, we want to determine if this subsequence is legitimate or not. (There are alternative ways of formulating the sequence recognition problem, for example, as an one-shot recognition process, as opposed to an incremental step-by-step recognition process as formulated here.)\n",
    "\n",
    "   With this formulation, sequence recognition can be turned into sequence generation/prediction, by basing recognition on prediction; that is, s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> → yes (a recognition problem), if and only if s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j-1</sub> → s<sup>p</sup><sub>j</sub> (a prediction problem) and s<sup>p</sup><sub>j</sub> = s<sup>a</sup><sub>j</sub>, where s<sup>p</sup><sub>j</sub> is the prediction and s<sup>a</sup><sub>j</sub> is the actual element.\n",
    "  \n",
    "  So **recognition** can be interpreted either as comparing the sequence elements one by one OR by the condition that the element before-last will predict the last element (this assumes that the last two elemrnts characterize the sequince i.e. that is what makes it unique). \n",
    "  \n",
    "  4. **Sequential decision making**\n",
    "  \n",
    "  That is, sequence generation through actions. \n",
    "  \n",
    "  there are several possible variations. In the goal oriented case, we have s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub>; s<sub>G</sub> → a<sub>j</sub>, where 1 ≤ i ≤ j < ∞; \n",
    "  that is, given s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> and the goal state s<sub>G</sub> , we want to choose an action a<sub>j</sub> at time step j that will likely lead to s<sub>G</sub> in the future. In the trajectory oriented case, we have s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub>; s<sub>j+1</sub> → a<sub>j</sub> , where 1 ≤ i ≤ j < ∞; that is, given s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> and the desired next state s<sub>j+1</sub>, we want to choose an action a j at time step j that will likely lead to s<sub>j+1</sub> in the next step. In the reinforcement maximizing case, we have s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> → a<sub>j</sub> , where 1 ≤ i ≤ j < ∞;\n",
    "that is, given s<sub>i</sub>, s<sub>i+1</sub>, ...., s<sub>j</sub> we want to choose an action a<sub>j</sub> at time step j that will likely lead to receiving maximum total reinforcement in the future. The calculation of total reinforcement can be in terms of discounted or undiscounted cumulative reinforcement, in terms of average reinforcement, or in terms of some other functions of reinforcement\n",
    "  \n",
    "The problems described so far are closed-loop i.e. find the next element.\n",
    "They can also be redefined as open-loop where we are interested the next N elements, not just the next one.\n",
    "\n",
    "There is a fifth problem when we want to recognize or make a decision for a group of similiar sequences. In this case we need a function that makes them invariant and compare the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
